{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week Seven: Data manipulation\n",
    "\n",
    "The goal of filling in the requested pieces is twofold: you should be able to run the worksheet and get the requested answer with the given dataset, and you should also be able to pass with different datasets (not given). These will often check unusual inputs, etc., so try to make sure all possible input datasets are accounted for.\n",
    "\n",
    "To be graded, your notebook must be runnable start to finish. If you can't make an in-notebook test pass, comment it out for to attempt to get partial credit. You should replace the `...` markers with your code. Do not change the names of the pre-defined variables and functions.\n",
    "\n",
    "Plots should have the required elements of a plot: labels, units if valid, a legend if more than one marker or line type is present. Titles are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import cauchy, expon\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Projections\n",
    "\n",
    "For this problem, you will read in the dataset here: `proj_data.csv` (sitting next to this notebook or from the given URL). You can use Pandas `read_csv`. It does not have an index, and the first row is the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/henryiii/compclass/master/problems/proj_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a collection of two vectors; `x_*` is displacement from the origin, and `d_*` is normalized direction. Note that `x_z` is 0, and so is not included explicitly in the file. Technically, a line only has 4 degrees of freedom, so you can reconstruct `d_z` from `d_x` and `d_y` using the relation $1 = d_x^2 + d_y^2 + d_z^2$.\n",
    "\n",
    "Add $d_z$ in a new `d_z` column, and a `x_z` column of zeros for $x_z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df.columns) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot a 2D histogram of $x_y$ vs. $x_x$. (Remember, it's convertional to write y axis vs. x axis when descibing a plot in text.) Plot 100 bins in the range -2 to 2 on both axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two vectors descibe a line made up of points $\\vec{p} = \\vec{x} + t \\vec{d}$ (and it the $\\vec{x}$ vectors currently lie on the $z=0$ plane). Project onto a plane at $z=+1$, and $z=-1$. Make 2D histograms like the one above for both planes. For this, it is *probably* easier to turn the three columns of length N describing x, y, and z into a 3xN numpy array (up to you). Depends on what you find easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Runners\n",
    "\n",
    "Let's load the marathon dataset from seaborn. After loading it, you should clean it up (or you can supply a converter dictionary to the read function, but cleaning it up afterwards is probably easier). Use `pd.to_timedelta` to convert the time columns, and make the gender a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mara = pd.read_csv('https://raw.githubusercontent.com/jakevdp/marathon-data/master/marathon-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mara.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity in the following plots, you can plot the timedelta columns as seconds. Use the `.dt` accessor to get access to the column's `total_seconds()`.\n",
    "\n",
    "Make a scatter plot of final vs. split times (seconds recommended). Draw a line that indicates where a runner would be if they finish the race in exactly twice the split time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A runner may run the first half slower than the second half; this is called a positive-split. If they run the second half faster, this is a negative split. Count the number of positive splits, negitave splits, and exactly equal runners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_RUNNERS = ...\n",
    "EQ_RUNNERS = ...\n",
    "NEG_RUNNERS = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\\\n",
    "Positive-splits: {POS_RUNNERS}\n",
    "Equal-splits:    {EQ_RUNNERS}\n",
    "Negitive-splits: {NEG_RUNNERS}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column for split fraction (`'split_fraction'`: 1 - second split time / first split time), then plot a 1D histogram over split fraction. Remember to convert to `total_seconds` before deviding, if you haven't already converted the columns. If you'd like, you can use a stacked histogram and plot different age brackets in different colors - I split into 15-25,25-35,...,85-95 (optional challenge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot a 2D histogram of age vs. split fraction. Are the variables correlated? What can you say about runners with a low finishing time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_2 = \"\"\"\n",
    "...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Tips\n",
    "\n",
    "Let's load the tips dataset from seaborn. After loading it, you should clean it up (or you can supply a converter dictionary to the read function, but cleaning it up afterwards is probably easier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the dataset types. `sex`, `day`, and `time` should be categorical. `smoker` should be boolean. You technically can do this in the `read_csv` function above instead. Warning: Note that this dataset has a column named \"size\": since there's a pandas dataframe member named `.size`, pandas will not let you use the shortcut `tips.size` to refer to the column, but rather it will be the `.size` member. You have been warned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add a column with the tip fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average tip fraction? What is the average tip fraction for male waiters from smokers in parties less than 3? You can just display the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVERAGE_TIP_TOTAL = ...\n",
    "AVERAGE_TIP_1 = ...\n",
    "print(AVERAGE_TIP_TOTAL, AVERAGE_TIP_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a normalized histogram of the `tip_frac` for male and female waiters with two histograms on the same axis (side by side bars, or points). Use 20 or fewer bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Generating a distribution\n",
    "\n",
    "Generate the following distribution two ways. The PDF is:\n",
    "\n",
    "$$\n",
    "P(x) = 1 - x^2\n",
    "$$\n",
    "\n",
    "From -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,100)\n",
    "y = 1 - x**2\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Method 1\n",
    "\n",
    "Use the rejection method generate a distribution. `N` is the maximum number of samples to generate (your function can produce less)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dist_1(N):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = generate_dist_1(1_000_000)\n",
    "plt.hist(vals, bins=np.linspace(-1,1,100), density=True)\n",
    "x = np.linspace(-1,1,100)\n",
    "y = (1 - x**2)/(4/3) # 4/3 = normalization factor\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Method 2\n",
    "\n",
    "Use the inverse CDF method to generate the distribution. You can calculate the CDF fairly easily. Note your work to calculate the CDF in a markdown cell, in comments or a docstring, or do it with sympy. For the inverse CDF, use an approximation, such as interpolation (unless you can invert the function symbolically, which I did not have much luck with). Remember to normalize the CDF to 1. (If you can't do this method, try using the binned technique from class).\n",
    "\n",
    "$$\n",
    "\\textrm{CDF}'(a) = \\int_{-1}^{a} f(x) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textrm{CDF}(y) = \\frac{\\textrm{CDF}'(a)}{\\textrm{CDF}'(1)} \n",
    "$$\n",
    "\n",
    "$$\n",
    "CDF'(a) = \\int_{-1}^{a} f(x) = x-\\frac{x^{3}}{3} \\biggr|_{x=-1}^{x=a}\n",
    "$$\n",
    "\n",
    "$$\n",
    "CDF'(a) = a - a^3/3 + 2/3 \n",
    "$$\n",
    "\n",
    "$$\n",
    "CDF(1) = 4/3\n",
    "$$\n",
    "\n",
    "$$\n",
    "CDF(a) = \\frac{CDF'(a)}{CDF'(1)} = \\frac{3}{4} a\n",
    "                                 - \\frac{1}{4} a^3\n",
    "                                 + \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "4 y = 3 a - a^3 + 2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dist_2(N):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_dist_2(1_000_000)\n",
    "plt.hist(data, bins=np.linspace(-1,1,101), density=True)\n",
    "x = np.linspace(-1,1,100)\n",
    "y = (1 - x**2)/(4/3) # 4/3 = normalization factor\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Unbinned fitting\n",
    "\n",
    "Fit the following unbinned dataset with a cauchy + an exponential distribution. You can also implement this yourself using `scipy.stats` for `cauchy` and `expon`. The range is from 0 to 20. The only tricky part is normalizing the PDFs, but you have the CDF, so it should be pretty easy.\n",
    "\n",
    "The cauchy PDF is:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\pi (1 + x^2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.loadtxt('Week7.Prob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(vals, bins=np.linspace(0,20,50), density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(params, x):\n",
    "    ...\n",
    "\n",
    "def nll_f(params, x):\n",
    "    return ... # Can be one line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fit, you should try initial parameters like `[10, 1, 10, .1]`. You probably should use bounds, with 0-1 for the fraction. The location of the cauchy can be constrained a bit too from the above plot by eye. I had the best luck with the `SLSQP` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data(data):\n",
    "    res = minimize(...)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fit_data(vals)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.x)\n",
    "plt.hist(vals, bins=np.linspace(0,20,50), density=True)\n",
    "x = np.linspace(0,20,100)\n",
    "y = f(res.x, x)\n",
    "plt.plot(x, y, color='C1')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
